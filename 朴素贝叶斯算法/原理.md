在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，
也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。
但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。

1. 朴素贝叶斯公式

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/1.png)

2. 朴素贝叶斯的模型

从统计学知识回到我们的数据分析。假如我们的分类模型样本是：

![2](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/2.png)

即我们有m个样本，每个样本有n个特征，特征输出有K个类别，定义为C1,C2,...,CK。
从样本我们可以学习得到朴素贝叶斯的先验分布P(Y=Ck)(k=1,2,...K),
接着学习到条件概率分布P(X=x|Y=Ck)=P(X1=x1,X2=x2,...Xn=xn|Y=Ck),然后我们就可以用贝叶斯公式得到X和Y的联合分布P(X,Y)了。联合分布P(X,Y)定义为：

![3](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/3.png)

从上面的式子可以看出P(Y=Ck)比较容易通过最大似然法求出，得到的P(Y=Ck)就是类别Ck在训练集里面出现的频数。但是P(X1=x1,X2=x2,...Xn=xn|Y=Ck)很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:从上面的式子可以看出P(Y=Ck)比较容易通过最大似然法求出，得到的P(Y=Ck)就是类别Ck在训练集里面出现的频数。
但是P(X1=x1,X2=x2,...Xn=xn|Y=Ck)很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:

![4](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/4.png)

从上式可以看出，这个很难的条件分布大大的简化了，但是这也可能带来预测的不准确性。你会说如果我的特征之间非常不独立怎么办?
如果真是非常不独立的话，那就尽量不要使用朴素贝叶斯模型了，考虑使用其他的分类方法比较好。(这也是贝叶斯算法“朴素”的原因)

最后回到我们要解决的问题，我们的问题是给定测试集的一个新样本特征，我们如何判断它属于哪个类型？
既然是贝叶斯模型，当然是后验概率最大化来判断分类了。我们只要计算出所有的K个条件概率P(Y=Ck|X=X(test)),然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测了。

3. 朴素贝叶斯的推断过程

对于P(Y=Ck),即样本类别Ck出现的次数mk除以样本总数m。
(a)对于P(Xj=Xa) 如果我们的Xj是离散的值，那么我们可以假设Xj符合多项式分布，这样得到P(Xj=X(test)j|Y=Ck) 是在样本类别Ck中，特征X(test)j出现的频率。即：|Y=Ck)(j=1,2,...n),这个取决于我们的先验条件：

![5](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/5.png)

其中mk为样本类别Ck总的特征计数，而mkj test为类别为Ck的样本中，第j维特征X(test)j出现的计数。
某些时候，可能某些类别在样本中没有出现，这样可能导致P(Xj=X(test)j|Y=Ck)为0，这样会影响后验的估计，为了解决这种情况，我们引入了拉普拉斯平滑，即此时有：

![5](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/6.png)

其中λ 为一个大于0的常数，常常取为1。Oj为第j个特征的取值个数。
(b)如果我们我们的Xj是非常稀疏的离散值，即各个特征出现概率很低，这时我们可以假设Xj符合伯努利分布，即特征Xj出现记为1，不出现记为0。
即只要Xj出现即可，我们不关注Xj的次数。这样得到P(Xj=X(test)j|Y=Ck) 是在样本类别Ck中，X(test)j出现的频率。此时有：

![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/7.png)

其中，X(test)j取值为0和1。

(c)如果我们我们的Xj是连续值，我们通常取Xj的先验概率为正态分布，即在样本类别Ck中，Xj的值符合正态分布。这样P(Xj=X(test)j|Y=Ck)的概率分布是：

![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/8.png)

其中μk和σ2k是正态分布的期望和方差，可以通过极大似然估计求得。μk为在样本类别Ck中，所有Xj的平均值。σ2k为在样本类别Ck中，所有Xj的方差。
对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。



 
