在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，
也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。
但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。

1. 朴素贝叶斯公式

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/1.png)

2. 朴素贝叶斯的模型

从统计学知识回到我们的数据分析。假如我们的分类模型样本是：

![2](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/2.png)

即我们有m个样本，每个样本有n个特征，特征输出有K个类别，定义为C1,C2,...,CK。
从样本我们可以学习得到朴素贝叶斯的先验分布P(Y=Ck)(k=1,2,...K),
接着学习到条件概率分布P(X=x|Y=Ck)=P(X1=x1,X2=x2,...Xn=xn|Y=Ck),然后我们就可以用贝叶斯公式得到X和Y的联合分布P(X,Y)了。联合分布P(X,Y)定义为：

![3](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/3.png)

从上面的式子可以看出P(Y=Ck)比较容易通过最大似然法求出，得到的P(Y=Ck)就是类别Ck在训练集里面出现的频数。但是P(X1=x1,X2=x2,...Xn=xn|Y=Ck)很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:从上面的式子可以看出P(Y=Ck)比较容易通过最大似然法求出，得到的P(Y=Ck)就是类别Ck在训练集里面出现的频数。
但是P(X1=x1,X2=x2,...Xn=xn|Y=Ck)很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:

![4](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/4.png)

从上式可以看出，这个很难的条件分布大大的简化了，但是这也可能带来预测的不准确性。你会说如果我的特征之间非常不独立怎么办?
如果真是非常不独立的话，那就尽量不要使用朴素贝叶斯模型了，考虑使用其他的分类方法比较好。(这也是贝叶斯算法“朴素”的原因)

最后回到我们要解决的问题，我们的问题是给定测试集的一个新样本特征，我们如何判断它属于哪个类型？
既然是贝叶斯模型，当然是后验概率最大化来判断分类了。我们只要计算出所有的K个条件概率P(Y=Ck|X=X(test)),然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测了。

3. 朴素贝叶斯的推断过程

对于P(Y=Ck),即样本类别Ck出现的次数mk除以样本总数m。
(a)对于P(Xj=Xa) 如果我们的Xj是离散的值，那么我们可以假设Xj符合多项式分布，这样得到P(Xj=X(test)j|Y=Ck) 是在样本类别Ck中，特征X(test)j出现的频率。即：|Y=Ck)(j=1,2,...n),这个取决于我们的先验条件：

![5](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/5.png)

其中mk为样本类别Ck总的特征计数，而mkj test为类别为Ck的样本中，第j维特征X(test)j出现的计数。
某些时候，可能某些类别在样本中没有出现，这样可能导致P(Xj=X(test)j|Y=Ck)为0，这样会影响后验的估计，为了解决这种情况，我们引入了拉普拉斯平滑，即此时有：

![5](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/6.png)

其中λ 为一个大于0的常数，常常取为1。Oj为第j个特征的取值个数。
(b)如果我们我们的Xj是非常稀疏的离散值，即各个特征出现概率很低，这时我们可以假设Xj符合伯努利分布，即特征Xj出现记为1，不出现记为0。
即只要Xj出现即可，我们不关注Xj的次数。这样得到P(Xj=X(test)j|Y=Ck) 是在样本类别Ck中，X(test)j出现的频率。此时有：

![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/7.png)

其中，X(test)j取值为0和1。

(c)如果我们我们的Xj是连续值，我们通常取Xj的先验概率为正态分布，即在样本类别Ck中，Xj的值符合正态分布。这样P(Xj=X(test)j|Y=Ck)的概率分布是：

![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/8.png)

其中μk和σ2k是正态分布的期望和方差，可以通过极大似然估计求得。μk为在样本类别Ck中，所有Xj的平均值。σ2k为在样本类别Ck中，所有Xj的方差。
对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。


5.  朴素贝叶斯算法过程

我们假设训练集为m个样本n个维度，如下：

![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/9.png)

共有K个特征输出类别，分别为C1,C2,...,CK,每个特征输出类别的样本个数为m1,m2,...,mK,
在第k个类别中，如果是离散特征，则特征Xj各个类别取值为mjl。其中l取值为1,2,...Sj，Sj为特征j不同的取值数。
输出为实例X(test)的分类

算法流程如下：
(1) 如果没有Y的先验概率，则计算Y的K个先验概率：P(Y=Ck)=(mk+λ)/(m+Kλ)，否则P(Y=Ck)为输入的先验概率。
(2) 分别计算第k个类别的第j维特征的第l个个取值条件概率：P(Xj=xjl|Y=Ck)
　　(a)如果是离散值:
  
    ![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/10.png)
    
    (b)如果是稀疏二项离散值:
    
    ![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/11.png)
    
    (c)如果是连续值不需要计算各个l的取值概率，直接求正态分布的参数:
    
    
    ![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/12.png)
    
（3）对于实例X(test)，分别计算：

    ![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/13.png)
    
（4）确定实例X(test)的分类Cresult

    ![6](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/images/14.png)





    






6.  朴素贝叶斯算法小结

朴素贝叶斯算法的主要原理基本已经做了总结，这里对朴素贝叶斯的优缺点做一个总结。
朴素贝叶斯的主要优点有：
（1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
（2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。
（3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。
朴素贝叶斯的主要缺点有：　　　
（1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
（2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。
（3）由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。
（4）对输入数据的表达形式很敏感。
 

 
