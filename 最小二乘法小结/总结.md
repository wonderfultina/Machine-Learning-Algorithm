1.最小二乘法的原理与要解决的问题　

举一个最简单的线性回归的简单例子，比如我们有m个只有一个特征的样本：
(x(1),y(1)),(x(2),y(2),...(x(m),y(m))
样本采用下面的拟合函数：
hθ(x)=θ0+θ1x
这样我们的样本有一个特征x，对应的拟合函数有两个参数θ0和θ1需要求出。
我们的目标函数为：

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/1.png)

用最小二乘法做什么呢，使J(θ0,θ1)最小，求出使J(θ0,θ1)最小时的θ0和θ1，这样拟合函数就得出了。
那么，最小二乘法怎么才能使J(θ0,θ1)最小呢？

2.最小二乘法的代数法解法

上面提到要使J(θ0,θ1)最小，方法就是对θ0和θ1分别来求偏导数，令偏导数为0，得到一个关于θ0和θ1的二元方程组。求解这个二元方程组，就可以得到θ0和θ1的值。

拟合函数表示为 hθ(x1,x2,...xn)=θ0+θ1x1+...+θnxn, 其中θi (i = 0,1,2... n)为模型参数，xi (i = 0,1,2... n)为每个样本的n个特征值。这个表示可以简化，我们增加一个特征x0=1 ，这样拟合函数表示为：

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/2.png)

损失函数表示为：

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/3.png)

利用损失函数分别对θi(i=0,1,...n)求导,并令导数为0可得：

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/4.png)

这样我们得到一个N+1元一次方程组，这个方程组有N+1个方程，求解这个方程，就可以得到所有的N+1个未知的θ。

3.最小二乘法的矩阵法解法

假设函数hθ(x1,x2,...xn)=θ0+θ1x1+...+θn−1xn−1的矩阵表达方式为：

hθ(x)=Xθ 

其中， 假设函数hθ(X)为mx1的向量,θ为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。

损失函数定义为

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/5.png)

其中Y是样本的输出向量，维度为mx1. 12在这主要是为了求导后系数为1，方便计算。
根据最小二乘法的原理，我们要对这个损失函数对θ向量求导取0。结果如下式：


![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/6.png)

这里面用到了矩阵求导链式法则，和两个个矩阵求导的公式。

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/7.png)

对上述求导等式整理后可得：

![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/8.png)


![1](https://github.com/wonderfultina/Machine-Learning-Algorithm/blob/master/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%B0%8F%E7%BB%93/images/9.png)

4.最小二乘法的局限性和适用场景　　

从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。

首先，最小二乘法需要计算XTX的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让XTX的行列式不为0，然后继续使用最小二乘法。

第二，当样本特征n非常的大的时候，计算XTX的逆矩阵是一个非常耗时的工作（nxn的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。


第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。


第四，讲一些特殊情况。当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。


